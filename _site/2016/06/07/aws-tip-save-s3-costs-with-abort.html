<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>AWS Tip: Save S3 costs with abort multipart lifecycle policy | Your awesome title</title>
<meta name="generator" content="Jekyll v4.3.4" />
<meta property="og:title" content="AWS Tip: Save S3 costs with abort multipart lifecycle policy" />
<meta name="author" content="Craig Watcham" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Introduction S3 multipart uploads provide a number of benefits&nbsp;--&nbsp;better throughput, recovery from network errors -- and a number of tools will automatically use multipart uploads for larger uploads. The AWS CLI cp, mv, and sync commands all make use of multipart uploads and make a&nbsp;note&nbsp;that &quot;If the process is interrupted by a kill command or system failure, the in-progress multipart upload remains in Amazon S3 and must be cleaned up manually...&quot; The reason you would want to clean up these failed multipart uploads is because you will be charged for the storage they use while waiting for the upload to be completed (or aborted). This post provides some detail on how to find the incomplete uploads and options for removing them to save storage costs. Finding incomplete multipart uploads If you have relatively few buckets or only want to check your biggest buckets (CloudWatch S3 metrics are useful for finding these) the AWS CLI s3api list-multipart-uploads&nbsp;command is a simple check: aws s3api list-multipart-uploads --bucket [bucket-name] No output indicates that the bucket does not contain any incomplete uploads, see the list-multipart-uploads documentation linked above for an example of the output on a bucket that does contain an incomplete upload. A simple bash script to check all your buckets:" />
<meta property="og:description" content="Introduction S3 multipart uploads provide a number of benefits&nbsp;--&nbsp;better throughput, recovery from network errors -- and a number of tools will automatically use multipart uploads for larger uploads. The AWS CLI cp, mv, and sync commands all make use of multipart uploads and make a&nbsp;note&nbsp;that &quot;If the process is interrupted by a kill command or system failure, the in-progress multipart upload remains in Amazon S3 and must be cleaned up manually...&quot; The reason you would want to clean up these failed multipart uploads is because you will be charged for the storage they use while waiting for the upload to be completed (or aborted). This post provides some detail on how to find the incomplete uploads and options for removing them to save storage costs. Finding incomplete multipart uploads If you have relatively few buckets or only want to check your biggest buckets (CloudWatch S3 metrics are useful for finding these) the AWS CLI s3api list-multipart-uploads&nbsp;command is a simple check: aws s3api list-multipart-uploads --bucket [bucket-name] No output indicates that the bucket does not contain any incomplete uploads, see the list-multipart-uploads documentation linked above for an example of the output on a bucket that does contain an incomplete upload. A simple bash script to check all your buckets:" />
<link rel="canonical" href="http://localhost:4000/2016/06/07/aws-tip-save-s3-costs-with-abort.html" />
<meta property="og:url" content="http://localhost:4000/2016/06/07/aws-tip-save-s3-costs-with-abort.html" />
<meta property="og:site_name" content="Your awesome title" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2016-06-07T17:41:00+01:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="AWS Tip: Save S3 costs with abort multipart lifecycle policy" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Craig Watcham"},"dateModified":"2016-06-07T17:41:00+01:00","datePublished":"2016-06-07T17:41:00+01:00","description":"Introduction S3 multipart uploads provide a number of benefits&nbsp;--&nbsp;better throughput, recovery from network errors -- and a number of tools will automatically use multipart uploads for larger uploads. The AWS CLI cp, mv, and sync commands all make use of multipart uploads and make a&nbsp;note&nbsp;that &quot;If the process is interrupted by a kill command or system failure, the in-progress multipart upload remains in Amazon S3 and must be cleaned up manually...&quot; The reason you would want to clean up these failed multipart uploads is because you will be charged for the storage they use while waiting for the upload to be completed (or aborted). This post provides some detail on how to find the incomplete uploads and options for removing them to save storage costs. Finding incomplete multipart uploads If you have relatively few buckets or only want to check your biggest buckets (CloudWatch S3 metrics are useful for finding these) the AWS CLI s3api list-multipart-uploads&nbsp;command is a simple check: aws s3api list-multipart-uploads --bucket [bucket-name] No output indicates that the bucket does not contain any incomplete uploads, see the list-multipart-uploads documentation linked above for an example of the output on a bucket that does contain an incomplete upload. A simple bash script to check all your buckets:","headline":"AWS Tip: Save S3 costs with abort multipart lifecycle policy","mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:4000/2016/06/07/aws-tip-save-s3-costs-with-abort.html"},"url":"http://localhost:4000/2016/06/07/aws-tip-save-s3-costs-with-abort.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@latest/css/all.min.css">
  <link rel="stylesheet" href="/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="http://localhost:4000/feed.xml" title="Your awesome title" />
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Your awesome title</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">AWS Tip: Save S3 costs with abort multipart lifecycle policy</h1>
    <p class="post-meta"><time class="dt-published" datetime="2016-06-07T17:41:00+01:00" itemprop="datePublished">
        Jun 7, 2016
      </time>â€¢ 
          <span itemprop="author" itemscope itemtype="http://schema.org/Person">
            <span class="p-author h-card" itemprop="name">Craig Watcham</span></span></p>
  </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h3>
Introduction</h3>
<div>
S3 multipart uploads provide a number of <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/uploadobjusingmpu.html">benefits</a>&nbsp;--&nbsp;better throughput, recovery from network errors -- and a number of tools will automatically use multipart uploads for larger uploads. The AWS CLI cp, mv, and sync commands all make use of multipart uploads and make a&nbsp;<a href="http://docs.aws.amazon.com/cli/latest/userguide/using-s3-commands.html#using-s3-commands-managing-objects">note</a>&nbsp;that "If the process is interrupted by a kill command or system failure, the in-progress multipart upload remains in Amazon S3 and must be cleaned up manually..."</div>
<div>
<br /></div>
<div>
The reason you would want to clean up these failed multipart uploads is because you will be charged for the storage they use while waiting for the upload to be completed (or aborted). This post provides some detail on how to find the incomplete uploads and options for removing them to save storage costs.</div>
<div>
<br /></div>
<h3>
Finding incomplete multipart uploads</h3>
<div>
If you have relatively few buckets or only want to check your biggest buckets (<a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/cloudwatch-monitoring.html">CloudWatch S3 metrics</a> are useful for finding these) the AWS CLI s3api <a href="http://docs.aws.amazon.com/cli/latest/reference/s3api/list-multipart-uploads.html">list-multipart-uploads</a>&nbsp;command is a simple check:</div>
<blockquote class="tr_bq">
<span style="font-family: &quot;georgia&quot; , &quot;times new roman&quot; , serif;">aws s3api list-multipart-uploads --bucket [bucket-name]</span></blockquote>
No output indicates that the bucket does not contain any incomplete uploads, see the list-multipart-uploads documentation linked above for an example of the output on a bucket that does contain an incomplete upload. A simple bash script to check all your buckets:<br />
<br />
<script src="https://gist.github.com/watchamcb/c412482f1767cf3c3daf08996271a81c.js"></script>

This script will list your buckets and the first page (out of possibly many more) incomplete multipart upload keys along with the date they were initiated. The region lookup is required to handle bucket names containing dots and buckets in eu-central-1 (SigV4).<br />
<br />
<h3>
Cleaning up</h3>
<div>
Once you have identified the buckets containing incomplete uploads it is worth investigating some of the recent failed uploads to see whether there is an underlying issue that needs to be addressed, particularly if the uploads relate to backups or important log files. The most typical cause is instances being terminated before completing uploads (look at <a href="http://docs.aws.amazon.com/autoscaling/latest/userguide/lifecycle-hooks.html">Lifecycle Hooks</a>&nbsp;to fix this if you are using Auto Scaling) but they may also be result of applications not performing cleanup on failures or not handling errors correctly.</div>
<div>
<br /></div>
<div>
A multipart upload can be aborted using the <a href="http://docs.aws.amazon.com/cli/latest/reference/s3api/abort-multipart-upload.html">abort-multipart-upload</a> s3api command in the AWS CLI using the object key and upload ID returned by list-multipart-uploads command. This can be scripted but will take time to complete for buckets containing large numbers of incomplete uploads, fortunately there is an easier way. S3 now supports a <a href="http://docs.aws.amazon.com/AmazonS3/latest/dev/mpuoverview.html#mpu-abort-incomplete-mpu-lifecycle-config">bucket lifecycle policy</a> to automatically delete incomplete uploads after a specified period of time. Enabling the policy in the AWS console is fairly quick and easy, see <a href="https://aws.amazon.com/blogs/aws/s3-lifecycle-management-update-support-for-multipart-uploads-and-delete-markers/">Jeff's blog post</a> for details. A rather messy boto3 example script for enabling the policy on all buckets can be found <a href="https://github.com/watchamcb/aws/blob/master/set-abort-multipart.py">here</a>, it should work with most bucket configurations but it comes with no guarantees and you use it at your own risk.</div>
<div>
<br /></div>
<h3>
Conclusion (why you should do this)</h3>
<div>
If you are using S3 to store large (&gt; 5MB) objects and you are spending more than a few dollars a month on S3 storage then there is a fairly good chance that you are paying unnecessarily for failed/incomplete multipart uploads. It should only take a few minutes to review your buckets and could potentially have significant monthly savings.</div>

  </div><a class="u-url" href="/2016/06/07/aws-tip-save-s3-costs-with-abort.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="http://localhost:4000/feed.xml">
            <svg class="svg-icon orange">
              <path d="M12.8 16C12.8 8.978 7.022 3.2 0 3.2V0c8.777 0 16 7.223 16 16h-3.2zM2.194
                11.61c1.21 0 2.195.985 2.195 2.196 0 1.21-.99 2.194-2.2 2.194C.98 16 0 15.017 0
                13.806c0-1.21.983-2.195 2.194-2.195zM10.606
                16h-3.11c0-4.113-3.383-7.497-7.496-7.497v-3.11c5.818 0 10.606 4.79 10.606 10.607z"
              />
            </svg><span>Subscribe</span>
          </a>
        </p>
        <ul class="contact-list">
          <li class="p-name">GitHub User</li>
          <li><a class="u-email" href="mailto:your-email@domain.com">your-email@domain.com</a></li>
        </ul>
      </div>
      <div class="footer-col">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"></ul>
</div>

  </div>

</footer>
</body>

</html>
